behaviors:
  SoccerTwos:
    trainer_type: ppo # 训练算法，可以选择 ppo 和 sac
    
    # 设置超参数
    hyperparameters: 
    
      batch_size: 2048   
      buffer_size: 20480     # 缓存区的大小
      learning_rate: 0.0003  # 学习率
      beta: 0.005  # 
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
      learning_rate_schedule: constant
      
    # 网络设置
    network_settings:    
      normalize: false           # 是否归一化
      hidden_units: 512         # MLP网络隐层神经元的数量
      num_layers: 2             # 网络多少层
      vis_encode_type: simple    # visual observation的编码， 在这里不需要设置
    
    # 奖励设置
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    
    # 多少个epoch， 保存checkpoint
    keep_checkpoints: 5
    
    # 训练的最大步数
    max_steps: 500000
    
    # experience的长度，约等于每一条轨迹的长度，常用值：32-2048， (值变大时，高方差、低偏差;  值变小时，低方差，高偏差)，当一条轨迹中的奖励密集或者轨迹非常非常大时，这个值可以设置的小一点。
    # 为了能够评估关键动作，这个值也不宜过小，需要在两者之中折中。
    time_horizon: 1000

    summary_freq: 10000
    threaded: false
    
    # self play， 训练AI vs AI，训练时，AI与自己对打，首先，训练一个基础模型 base1, 然后固定住base1, 训练base2与base1对打，迭代一定轮数之后，如果base2优于base1, 固定base2, 训练base3与base2对打，以此类推
    self_play:
      save_steps: 50000           # 多少步保存模型
      team_change: 200000         # 多少步之后，固定一个版本的模型
      swap_steps: 2000
      window: 10       # 保存多少个旧的模型
      
      # 与最新模型对战的概率，从window个池子中随机选取对手
      play_against_latest_model_ratio: 0.5
      
      # 初始elo分值
      initial_elo: 1200.0
